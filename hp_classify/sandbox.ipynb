{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SANDBOX for code development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "#import custom modules\n",
    "import prep.prep_data as prep\n",
    "import prep.prep_cv as cv\n",
    "import model.fuzzy as fz\n",
    "\n",
    "#magik\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup globals\n",
    "#setup directories\n",
    "CWD = os.getcwd()\n",
    "HOME_DIR = os.path.abspath(os.path.join(CWD, os.pardir))\n",
    "DATA_DIR = HOME_DIR + \"/data\"\n",
    "DATA_FILENAME = \"example_data.csv\"\n",
    "RESULTS_DIR = HOME_DIR + \"/results\"\n",
    "\n",
    "#setup lists of vars to work with\n",
    "STR_VARS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "NUM_VARS = [s + '_num' for s in STR_VARS]\n",
    "RANK_VARS = [s + '_rank' for s in STR_VARS]\n",
    "\n",
    "#which variable do you want to predict (currently: floor/wall/roof)\n",
    "DEP_VAR = \"housing_roof\"\n",
    "PRED_VAR = DEP_VAR + \"_rank\" #will always be using the strings to predict ranking\n",
    "\n",
    "#setup a filter to select which surveys you want to work with\n",
    "SVY_FILTER = ['MACRO_DHS']\n",
    "\n",
    "#analytical options\n",
    "CV_SAMPLE_PCT = .2 #hold out x% for testing\n",
    "CV_SAMPLE_WT = \"N\" #which variable(if any) shall weight your test sample\n",
    "CV_FOLDS = 2 #use a x-fold cross-validation env\n",
    "\n",
    "#garbage lists\n",
    "STR_GARBAGE = ['nan', 'other', 'not a dejure resident', 'not dejure resident']\n",
    "RANK_GARBAGE = ['4', '5', '6', '7', '8', '9', 'n']\n",
    "\n",
    "#dictionaries\n",
    "PRED_DICT = {'natural':'1', 'rudimentary':'2', 'finished':'3'} #map categories back to ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~begin reading\n",
      "data read!\n",
      "~begin cleaning\n",
      "data clean!\n",
      "~applying filter\n"
     ]
    }
   ],
   "source": [
    "df = prep.read_then_clean(DATA_DIR + \"/\" + DATA_FILENAME, STR_VARS, SVY_FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nan': nan, 'other': nan, 'not a dejure resident': nan, 'not dejure resident': nan}\n",
      "removing garbage from  housing_roof\n",
      "removing garbage from  housing_wall\n",
      "removing garbage from  housing_floor\n",
      "defining ranking for  housing_roof_num\n",
      "defining ranking for  housing_wall_num\n",
      "defining ranking for  housing_floor_num\n",
      "{'4': nan, '5': nan, '6': nan, '7': nan, '8': nan, '9': nan, 'n': nan}\n",
      "removing garbage from  housing_roof_rank\n",
      "removing garbage from  housing_wall_rank\n",
      "removing garbage from  housing_floor_rank\n",
      "sampling df, iteration # 0\n",
      "sampling df, iteration # 1\n"
     ]
    }
   ],
   "source": [
    "df_clean = prep.remove_garbage_codes(df, STR_VARS, STR_GARBAGE)\n",
    "df_clean = prep.extract_ranking(df_clean, NUM_VARS)\n",
    "df_clean = prep.remove_garbage_codes(df_clean, RANK_VARS, RANK_GARBAGE)\n",
    "train_list = cv.cv_censor_col(df_clean, PRED_VAR, CV_SAMPLE_PCT, CV_SAMPLE_WT, CV_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "cv loop:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on cv loop # 0\n",
      "building corpus for rank # 1\n",
      "building corpus for rank # 2\n",
      "building corpus for rank # 3\n",
      "extracting unknown strings\n",
      "need to classify 285 unknown strings\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='classifying unknown strings', max=285, style=ProgressStyle(deâ€¦"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing... cement bricks\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n",
      "analyzing... sod mud with grass\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n",
      "analyzing... wood planks\n",
      "~>corpus# 0\n",
      "~>corpus# 1\n",
      "~>corpus# 2\n",
      "analyzing... wood timber\n",
      "~>corpus# 0\n",
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5b32ad7940ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#run your cross-validation analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv_distrib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuzzy_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEP_VAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRED_DICT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#output the results to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# save_results_df(cv_results, out_dir, \"cv_results\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-378200f44a30>\u001b[0m in \u001b[0;36mfuzzy_cv\u001b[0;34m(cv_list, base_var, rank_dictionary, subset, threshold, jupyter)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#find distribution of scores for each string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mdistrib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuzzy_scan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midk_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#TODO, output plots of distribution for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/_code/school/cse583/hpc-housing-quality/hp_classify/model/fuzzy.py\u001b[0m in \u001b[0;36mfuzzy_scan\u001b[0;34m(unknown_list, corpus_list)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#loop over each word and compute the similarity score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRatio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#append scores to create a distribution for the entire corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hpc/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py\u001b[0m in \u001b[0;36mWRatio\u001b[0;34m(s1, s2, force_ascii, full_process)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptsor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mtsor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_sort_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munbase_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mtser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_set_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munbase_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/hpc/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py\u001b[0m in \u001b[0;36mtoken_sort_ratio\u001b[0;34m(s1, s2, force_ascii, full_process)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtoken_sort_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \"\"\"Return a measure of the sequences' similarity between 0 and 100\n\u001b[1;32m    103\u001b[0m     \u001b[0mbut\u001b[0m \u001b[0msorting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mcomparing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run your cross-validation analysis\n",
    "cv_distrib, cv_preds, cv_results, cv_df = fuzzy_cv(train_list, DEP_VAR, PRED_DICT)\n",
    "\n",
    "#output the results to csv\n",
    "# save_results_df(cv_results, out_dir, \"cv_results\")\n",
    "# save_results_df(cv_preds, out_dir, \"cv_preds\")\n",
    "# save_results_df(cv_df, out_dir, \"cv_df\")\n",
    "# save_results_df(cv_distrib, out_dir, \"cv_distrib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loop over all cross-validation results and plot them in chunks of 26 (4colsx6rows)\n",
    "\n",
    "#plot results\n",
    "fz.fuzzy_density(cv_distrib, 'word', \n",
    "                 ['natural', 'rudimentary', 'finished'],\n",
    "                 color_list={'natural':'r', 'rudimentary':'b', 'finished':'g'},\n",
    "                 cutoff=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_to_pdf(df, graph, graph_dir, graph_filename, graph_title):\n",
    "    \"\"\"\n",
    "    This is a demo of creating a pdf file with several pages,\n",
    "    as well as adding metadata and annotations to pdf files.\n",
    "    \"\"\"\n",
    "\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    #build filepath\n",
    "    pdf_filepath = graph_dir + \"/\" + graph_filename\n",
    "\n",
    "    # Create the PdfPages object to which we will save the pages:\n",
    "    # The with statement makes sure that the PdfPages object is closed properly at\n",
    "    # the end of the block, even if an Exception occurs.\n",
    "    with PdfPages(pdf_filepath) as pdf:\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.title(graph_title)\n",
    "        pdf.savefig(graph)  # or you can pass a Figure object to pdf.savefig\n",
    "        plt.close()\n",
    "\n",
    "        # We can also set the file's metadata via the PdfPages object:\n",
    "        d = pdf.infodict()\n",
    "        d['Title'] = 'Multipage PDF Example'\n",
    "        d['Author'] = u'Jouni K. Sepp\\xe4nen'\n",
    "        d['Subject'] = 'How to create a multipage pdf file and set its metadata'\n",
    "        d['Keywords'] = 'PdfPages multipage keywords author title subject'\n",
    "        d['CreationDate'] = datetime.datetime(2009, 11, 13)\n",
    "        d['ModDate'] = datetime.datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/cv.py\n"
     ]
    }
   ],
   "source": [
    "#%%file ./model/cv.py\n",
    "\n",
    "def fuzzy_cv(cv_list, base_var, rank_dictionary, subset=None, threshold=75, jupyter=False):\n",
    "\n",
    "    #import packages\n",
    "    from fuzzywuzzy import fuzz\n",
    "    from fuzzywuzzy import process\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    if jupyter == True:\n",
    "        from tqdm import tqdm_notebook as tqdm\n",
    "    else: \n",
    "        from tqdm import tqdm as tqdm\n",
    "    \n",
    "    #import custom modules\n",
    "    import sys\n",
    "    sys.path.append('../hp_classify')\n",
    "    import model.fuzzy as fz\n",
    "    \n",
    "    #setup objects\n",
    "    rank_var = base_var + '_rank'\n",
    "    og_var = rank_var + '_og'\n",
    "    \n",
    "    #TODO validate syntax\n",
    "    rank_values = list(rank_dictionary.values())\n",
    "    rank_keys = list(rank_dictionary.keys())\n",
    "    \n",
    "    #create lists to store loop outputs\n",
    "    cv_distrib = []\n",
    "    cv_preds = []\n",
    "    cv_results = []\n",
    "    cv_df = []\n",
    "    \n",
    "    #loop over each cross validation:\n",
    "    for i in tqdm(range(len(cv_list)), desc=\"cv loop\"):\n",
    "        \n",
    "        print('working on cv loop #', i)\n",
    "        df = cv_list[i].copy() #subset the cv list to the current df\n",
    "\n",
    "        #build corpus of known and unknown strings\n",
    "        str_list, idk_strings = fz.build_corpus(df, base_var, rank_var, rank_values)\n",
    "        \n",
    "        #subset the unknown strings to allow for faster testing\n",
    "        if subset != None:\n",
    "            idk_strings = idk_strings[subset]\n",
    "        \n",
    "        #find distribution of scores for each string\n",
    "        distrib = fz.fuzzy_scan(idk_strings, str_list)\n",
    "        \n",
    "        #TODO, output plots of distribution for analysis\n",
    "\n",
    "        \n",
    "        #predict class based on probability of exceeding similarity cutoff\n",
    "        preds = fz.fuzzy_predict(distrib, rank_keys, 'word', threshold,\n",
    "                                 rank_dictionary)\n",
    "\n",
    "        #merge results back on the test data to validate\n",
    "        out = df[df['train']==0]\n",
    "        out = pd.merge(out,\n",
    "                       preds,\n",
    "                       left_on=base_var,\n",
    "                       right_on='word',\n",
    "                       how='left')\n",
    "\n",
    "        #calculate success rate and tabulate\n",
    "        out['success'] = np.where(out[og_var] == out['pred'], 1, 0)\n",
    "        success_rate = pd.crosstab(out[~pd.isnull(out['pred'])]['success'], columns='count')\n",
    "        \n",
    "        #append results to prep for next loop\n",
    "        cv_distrib.append(distrib)\n",
    "        cv_preds.append(preds)\n",
    "        cv_results.append(success_rate)\n",
    "        cv_df.append(out)\n",
    "        \n",
    "    return(cv_distrib, cv_preds, cv_results, cv_df)\n",
    "\n",
    "\n",
    "def save_results_df(df, out_dir, out_name):\n",
    "    \n",
    "    out_path = f'{out_dir}//{out_name}.csv'    \n",
    "    print('saving df to', out_path)\n",
    "    \n",
    "    df = pd.concat(df)\n",
    "    df.to_csv(out_path, header=False, sep=';')\n",
    "    \n",
    "    return(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/fuzzy.py\n"
     ]
    }
   ],
   "source": [
    "#%%file ./model/fuzzy.py\n",
    "\n",
    "#define function to replace meaningless values with NaNs\n",
    "# def extract_ranking(df, vars_to_clean):\n",
    "#     \"\"\"This helper function is used to \n",
    "\n",
    "#     Args:\n",
    "#     df (pandas df): This is a pandas df that has \n",
    "#     dep_var (str): This is the name of a column\n",
    "\n",
    "#     Returns:\n",
    "#         df_out: \n",
    "        \n",
    "#     TODO: ?\n",
    "\n",
    "#     \"\"\"\n",
    "        \n",
    "#     df_out = df.copy()\n",
    "\n",
    "#     #output a clean dataset\n",
    "#     return \n",
    "\n",
    "def build_corpus(df, str_var, rank_var, rank_list):\n",
    "    \n",
    "    \"\"\"\" This function organizes the materials for which the rank is known into three broad categories of material quality:\n",
    "    natural, rudimentary, finished. \n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): This is a panda DataFrame containing the distribution of the similarity scores\n",
    "        str_var (list of str): This is a list of the strings for which the rank is known. The strings are classified\n",
    "        within one of three categories of materials.\n",
    "        rank_var (list of int):\n",
    "        rank_list (list of int):\n",
    "        \n",
    "    Returns:\n",
    "        distrib: The distribution of the similarity scores between each unknown material in the unknown list and known material\n",
    "        in the corpus_list.\n",
    "    \"\"\" \n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from fuzzywuzzy import fuzz\n",
    "    from fuzzywuzzy import process\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for x in rank_list:\n",
    "        print(\"building corpus for rank #\", x)\n",
    "        out.append(df[df[rank_var]==x][str_var].values)    \n",
    "\n",
    "    print(\"extracting unknown strings\")\n",
    "    other = df[~df[rank_var].isin(rank_list)][str_var].unique()\n",
    "    other = other[~pd.isnull(other)] #cant classify NaN\n",
    "    print(\"need to classify\", len(other), \"unknown strings\")\n",
    "\n",
    "    return(out, other)\n",
    "\n",
    "def fuzzy_scan(unknown_list, corpus_list):\n",
    "    \n",
    "    \"\"\"\" This function takes a list of \"unknown\"  materials (i.e. materials outside our list of materials whose rank is known)\n",
    "    as input and compare them to our corpus of known materials. The comparison is based on the computation of a score.\n",
    "    The score reflects how similar the unknow strings are to each material within each of the three corpus.\n",
    "\n",
    "    Args:\n",
    "        unknown_list (list of str): This is a list of strings whose rank is unknown\n",
    "        corpus_list (list of str): This is a list of the strings for which the rank is known. The strings are classified\n",
    "        within one of three categories of materials.\n",
    "\n",
    "    Returns:\n",
    "        distrib (DataFrame): The distribution of the similarity scores between each unknown material in the unknown list and known material\n",
    "        in the corpus_list.\n",
    "    \"\"\" \n",
    "    \n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from fuzzywuzzy import fuzz\n",
    "    from fuzzywuzzy import process\n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "    distrib = []\n",
    "\n",
    "    #loop over each unknown string\n",
    "    for x in tqdm_notebook(range(len(unknown_list)), desc=\"classifying unknown strings\", leave=False): \n",
    "        unknown_str = unknown_list[x]\n",
    "        print('analyzing...', unknown_str)\n",
    "\n",
    "        out = []\n",
    "        #loop over each corpus to compute similarity scores for all words in a given housing quality score\n",
    "        for y in range(len(corpus_list)):\n",
    "            print('~>corpus#', y)\n",
    "            corpus = corpus_list[y]\n",
    "\n",
    "\n",
    "            scores = []\n",
    "            #loop over each word and compute the similarity score\n",
    "            for z in range(len(corpus)):\n",
    "                scores.append(fuzz.WRatio(unknown_str, corpus[z]))\n",
    "\n",
    "            out.append(scores) #append scores to create a distribution for the entire corpus\n",
    "\n",
    "        #append distributions of scores\n",
    "        distrib.append(pd.DataFrame({'word': unknown_str, \n",
    "                                     'natural':pd.Series(out[0]), \n",
    "                                     'rudimentary':pd.Series(out[1]), \n",
    "                                     'finished':pd.Series(out[2]) #note series method used to overcome differing lengths\n",
    "                                    }))\n",
    "\n",
    "\n",
    "    return(pd.concat(distrib))\n",
    "\n",
    "def fuzzy_predict(df, var_list, grouping, cutoff, dictionary):\n",
    "    \n",
    "    \"\"\" This function takes the distribution of the similarity scores between each unknown material in the unknown list\n",
    "    and known material in the corpus_list, and based on this distribution predicts the rank of each unknown material.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): This is a panda DataFrame containing the distribution of the similarity scores\n",
    "        between each unknown material in the unknown list and known material in the corpus_list\n",
    "        var_list (list of str): This is the list of the three broad categories of materials - natural, rudimentary, finished.\n",
    "        grouping : the column to group the scores on.\n",
    "        cutoff : a threshold chosen by the user to exclude the \"noise\" stemming from the little scores, which only\n",
    "        share a few letters in common with the unknown material of interest\n",
    "        dictionary : a dictionary that maps the three categories of materials - natural, rudimentary, finished, with an\n",
    "        ordinal value - 1, 2, 3.\n",
    "    Returns:\n",
    "        out: The list of predicted ranks for each unknown material.\n",
    "    \"\"\" \n",
    "\n",
    "    #calculate the probability that a classification score exceeds cutoff\n",
    "    out = df.groupby(grouping)[var_list].apply(lambda c: (c>cutoff).sum()/len(c))\n",
    "    \n",
    "    #return column w/ max value and map to rank with dictionary\n",
    "    out['pred'] = out[var_list].idxmax(axis=1).map(dictionary) \n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def fuzzy_transform(df, var_list, grouping, fx, stub):\n",
    "\n",
    "    for var in var_list:\n",
    "\n",
    "        print('calculating prob for...', var)\n",
    "\n",
    "        kwargs = {var+stub : lambda x: x[var]/x.groupby(grouping)[var].transform(fx)}\n",
    "        df = df.assign(**kwargs)\n",
    "\n",
    "    return(df)\n",
    "\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file ./prep/prep_cv.py\n",
    "\n",
    "#define necessary helper functions\n",
    "def cv_censor_col(df, colname, pct=.2, weight_var=None, reps=5):\n",
    "    \n",
    "    \"\"\"This function is used to create pandas dfs where a specified % of the values in a column have been censored\n",
    "    and replaced with NaN, so that they can be predicted in a cross-validation methodology. It returns a list of such\n",
    "    dfs that is the length of the reps argument.\n",
    "\n",
    "    Args:\n",
    "        df (pandas df): This is a pandas df that has columns with garbage values to be removed.\n",
    "        colname (str): This is a string indicating the name of a column that you want to censor and later predict.\n",
    "        pct (float): This is a value between 0-1 that indicates the fraction of values you want to censor. Default = 20%\n",
    "        weight_var (str): This is a string indicating the column name is used to weighted the sample. Default = No weight.\n",
    "        reps (int): This is an integer indicating the number of different training datasets to create. Default = 5x\n",
    "\n",
    "    Returns:\n",
    "        df_clean: This function returns a pandas df where the garbage codes have been replaced with NaN.\n",
    "        \n",
    "    TODO: ?\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #import packages\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for x in range(reps):\n",
    "            \n",
    "        print(\"sampling df, iteration #\", x)\n",
    "    \n",
    "        #first archive your old column in order to test later\n",
    "        new_df = df.copy()\n",
    "        new_df[colname + '_og'] = new_df[colname]\n",
    "        new_df['train'] = 1 #set column to specify whether training or test data\n",
    "\n",
    "        #draw a weighted sample if weight var is specified\n",
    "        if weight_var != None:\n",
    "            df_censor = new_df.sample(frac=pct, weights=weight_var)\n",
    "        else:\n",
    "            df_censor = new_df.sample(frac=pct)\n",
    "            \n",
    "        #now replace the sampled column with missing values in order to try and predict\n",
    "        #note that replacement is only done on the sampled indices\n",
    "        df_censor['train'] = 0 #note that this sample is no longer training data (it is test)\n",
    "        df_censor[colname] = \"replace_me\"\n",
    "        new_df.update(df_censor, overwrite=True)\n",
    "        new_df[colname].replace(\"replace_me\", np.nan, inplace=True)\n",
    "        #TODO unsure if this is pythonic method but it seems like df.update won't replace values with NaN, \n",
    "        #as such, need to do this workaround\n",
    "        \n",
    "        #store the result (df with columns censored)\n",
    "        out.append(new_df)\n",
    "    \n",
    "    #return the list of sampled dfs\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file ./prep/prep_data.py\n",
    "#define necessary helper functions\n",
    "def clean_text(text):\n",
    "    \"\"\"This function is used to clean a selection of text. \n",
    "    It uses several regular expressions and built in text commands in order to remove commonly seen \n",
    "    errors,\n",
    "    nonsense values, \n",
    "    punctuation, \n",
    "    digits, and \n",
    "    extra whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): This is a text value that needs to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        text: This function returns a cleaned version of the input text.\n",
    "        \n",
    "    TODO: Add functionality to impute a selected value for NaN or missing values?\n",
    "\n",
    "    \"\"\"\n",
    "    #import necessary modules\n",
    "    import re\n",
    "    \n",
    "    #force all vals in series to string\n",
    "    text = str(text)\n",
    "    \n",
    "    #first remove uppercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove common errors\n",
    "    text = re.sub(r\"\\[.]\", \"\", text) \n",
    "    text = re.sub(r\"\\<ff>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<fb>\", \"\", text)\n",
    "    text = re.sub(r\"\\<a\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<c\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<d\\d>\", \"\", text)\n",
    "    text = re.sub(r\"\\<e\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<f\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)   \n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "    # remove any remaining digit codes\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # remove any leading/trailing/duplicate whitespace\n",
    "    text = re.sub(' +', ' ', text.strip())\n",
    "    \n",
    "    return text\n",
    "    \n",
    "#define master function\n",
    "def read_then_clean(file_path, vars_to_clean, filter_series=None):\n",
    "    \"\"\"This is the master function for this module. It uses the previously defined helper functions,\n",
    "    in order to output a clean dataset for user. It reads in a selected .csv file from a given filepath,\n",
    "    and applies the previously defined cleaning functions to a list of variables provided by user.\n",
    "    \n",
    "    It can also optionally filter the df based on the survey series or TODO language.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): This is a string indicating which file that you want to read in.\n",
    "        vars_to_clean (list): This is a list of strings that indicate which columns you want to clean.\n",
    "        filter_series (list): This is a list of strings that indicate which survey series to keep.\n",
    "\n",
    "    Returns:\n",
    "        df_clean: This is a pandas df that has columns of text values that have been cleaned using the helper function.\n",
    "        \n",
    "    TODO: Is it better to return an obj called df_clean to be more explicit to user?\n",
    "\n",
    "    \"\"\"\n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #read in your data\n",
    "    print(\"~begin reading\")\n",
    "    df_raw = pd.read_csv(file_path, low_memory=False)\n",
    "    min_nrow = len(df_raw) #save the row count to test after cleaning and verify that rows are not being dropped\n",
    "    print(\"data read!\")\n",
    "    \n",
    "    #cleanup\n",
    "    print(\"~begin cleaning\")\n",
    "    df_clean = df_raw.copy()\n",
    "    for var in vars_to_clean:\n",
    "        df_clean[var] = df_clean[var].apply(clean_text)\n",
    "    print(\"data clean!\")\n",
    "    \n",
    "    # Verify that the minimum rowcount continues to be met\n",
    "    if len(df_clean) < min_nrow:\n",
    "        class RowCountException(Exception):\n",
    "            \"\"\"Custom exception class.\n",
    "            \n",
    "            This exception is raised when the minimum row is unmet.\n",
    "\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        raise RowCountException(\"Minimum number of rows were not returned after cleaning. Data is being lost!\")\n",
    "        \n",
    "    # Filter data if filter arguments are provided by user\n",
    "    if filter_series != None:\n",
    "        print(\"~applying filter\")\n",
    "        df_clean = df_clean[df_clean['survey_series'].isin(filter_series)]\n",
    "        \n",
    "    #output a clean dataset\n",
    "    return df_clean\n",
    "\n",
    "#define function to replace meaningless values with NaNs\n",
    "def remove_garbage_codes(df, vars_to_clean, garbage_list):\n",
    "    \"\"\"This helper function is used to remove garbage values from a pandas df, replacing them with NaN.\n",
    "\n",
    "    Args:\n",
    "    df (pandas df): This is a pandas df that has columns with garbage values to be removed.\n",
    "    vars_to_clean (list): This is a list of strings that indicate which columns you want to clean.\n",
    "    garbage_list (list): This is a list of strings that indicate which garbage values to replace with NaN\n",
    "\n",
    "    Returns:\n",
    "        df_clean: This function returns a pandas df where the garbage codes have been replaced with NaN.\n",
    "        \n",
    "    TODO: set up an inverse argument so you can have opt to pass acceptable codes and NaN all others\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # build dictionary to map all garbage values to NaN\n",
    "    garb_dict = {}\n",
    "    for string in garbage_list:\n",
    "        garb_dict[string] = np.nan\n",
    "    \n",
    "    print(garb_dict)\n",
    "    \n",
    "    for var in vars_to_clean:\n",
    "        print(\"removing garbage from \", var)\n",
    "        df_clean[var].replace(garb_dict, inplace=True)\n",
    "        \n",
    "    #output a clean dataset\n",
    "    return df_clean\n",
    "\n",
    "#define function to replace meaningless values with NaNs\n",
    "def extract_ranking(df, vars_to_clean):\n",
    "    \"\"\"This helper function is used to extract the ordinal rankings from numerical coding.\n",
    "\n",
    "    Args:\n",
    "    df (pandas df): This is a pandas df that has columns with garbage values to be removed.\n",
    "    vars_to_rank (list): This is a list of strings that indicate which columns you want to extract ranks from.\n",
    "\n",
    "    Returns:\n",
    "        df_out: This function returns a pandas df with new vars added with the ordinal rank cols defined.\n",
    "        \n",
    "    TODO: ?\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    \n",
    "    for var in vars_to_clean:\n",
    "        print(\"defining ranking for \", var)\n",
    "        newcol = re.sub(\"_num\", \"_rank\", var) \n",
    "        df_out[newcol] = df_out[var].astype(str).str[0]\n",
    "\n",
    "    #output a clean dataset\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tests/test_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%file ./tests/test_prep.py\n",
    "#write tests\n",
    "\"\"\"This is a module used to test a module: \"prep.py\" and its relevant functions read_then_clean and clean_text\n",
    "\n",
    "read_then_clean is a function that takes a csv with messy string values and \n",
    "creates then cleans a pandas df\n",
    "using clean_text\n",
    "\n",
    "This module tests that function by ensuring that it returns expected exceptions and\n",
    "does not contain unexpected values.\n",
    "\n",
    "This module also uses the opportunity of having the df loaded to tests the \n",
    "functions later in the data cleaning pipeline, including \n",
    "remove_garbage_codes, which removes unacceptable values and replaces them with NaN\n",
    "and extract_ranking, which generates the ordinal ranking variable from an input numerical code\n",
    "\"\"\"\n",
    "# import packages\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#import custom modules fpr testing\n",
    "import sys \n",
    "sys.path.append('.')\n",
    "import prep.prep_data as prep\n",
    "\n",
    "#set globals for tests\n",
    "#set globals for tests\n",
    "FILEPATH = '../data/housing_data.csv'\n",
    "CLEAN_COLS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "\n",
    "DIGITS = str([str(x) for x in range(100 + 1)])\n",
    "PUNCT = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "SPACE = '     '\n",
    "\n",
    "# if you compile the regex string first, it's even faster\n",
    "re_dig = re.compile('\\d')\n",
    "re_punct = re.compile('\\W+')\n",
    "re_white = re.compile(' +')\n",
    "\n",
    "STR_VARS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "NUM_VARS = [s + '_num' for s in STR_VARS]\n",
    "RANK_VARS = [s + '_rank' for s in STR_VARS]\n",
    "\n",
    "STR_GARBAGE = ['nan', 'other', 'not a dejure resident', 'not dejure resident']\n",
    "RANK_GARBAGE = ['4', '5', '6', '7', '8', '9', 'n']\n",
    "\n",
    "#read in the df using our function in order to pass to later tests\n",
    "#read in df using your function and then using pandas regular csv read, then compare the resulting dfs\n",
    "df = prep.read_then_clean(FILEPATH, CLEAN_COLS)\n",
    "raw_csv = pd.read_csv(FILEPATH)\n",
    "\n",
    "#also passed it through the rest of the cleaning pipeline on order to compare df to df_clean\n",
    "df_clean = prep.remove_garbage_codes(df, STR_VARS, STR_GARBAGE)\n",
    "df_clean = prep.extract_ranking(df_clean, NUM_VARS)\n",
    "df_clean = prep.remove_garbage_codes(df_clean, RANK_VARS, RANK_GARBAGE)\n",
    "\n",
    "def test_globals():\n",
    "    \"\"\"This function tests that the test globals are properly defined.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(DIGITS) != None, \"global doesn't contain digits!\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(PUNCT) != None, \"global doesn't contain punctuation!\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(SPACE) != None, \"global doesn't contain whitespace!\"\n",
    "    \n",
    "\n",
    "def test_clean_text():\n",
    "    \"\"\"This function tests that the clean text function is doing its job.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(prep.clean_text(DIGITS)) == None, \"clean_text did not remove the digits from test global.\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(prep.clean_text(PUNCT)) == None, \"clean_text did not remove the punctuation from test global.\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(prep.clean_text(SPACE)) == None, \"clean_text did not remove the whitespace from test global.\"\n",
    "\n",
    "def test_read_then_clean():\n",
    "    \"\"\"This function tests our master function and the subsquent data cleaning pipeline.\n",
    "    \"\"\"    \n",
    "    #assert that our function did not add or remove rows\n",
    "    assert len(raw_csv) == len(df), \"read_then_clean function is modifying the original csv's length\"\n",
    "    assert len(df.columns) == len(raw_csv.columns), \"read_then_clean function is modifying the original csv's width\"\n",
    "    \n",
    "    #assert that our initial read function cleaned up the strings in the columns we provided\n",
    "    #TODO: this test will fail if the columns were entirely clean to begin with (is this possible?)\n",
    "    for x in CLEAN_COLS:\n",
    "        assert (set(df[x].unique()) == set(raw_csv[x].unique())) == False, \"string columns are unmodified\"\n",
    "\n",
    "def test_cleaning_pipeline():\n",
    "    \"\"\"This function tests our cleaning pipeline to make sure that \n",
    "    garbage values are removed and ranks are create\n",
    "    \"\"\" \n",
    "    #assert that rankings were generated in the next step of the pipeline\n",
    "    for x in RANK_VARS:\n",
    "        #verify that it wasnt originally present in df\n",
    "        assert (x in df) == False, \"rank column present in raw data\"\n",
    "        #assert that this column was added \n",
    "        assert x in df_clean, \"rank column was not added by extract_ranking fx\"\n",
    "        \n",
    "    #assert that garbage was removed \n",
    "    for x in STR_VARS:\n",
    "        for y in STR_GARBAGE:\n",
    "            print(x, y)\n",
    "            #assert that it is removed\n",
    "            assert (y in df_clean[x].unique()) == False, \"garbage values not removed from clean dataframe\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./tests/test_model.py\n",
    "#write tests\n",
    "\"\"\"This is a module used to test a module: \"model.py\" and its relevant functions x and y\n",
    "\n",
    "xx is a function that \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# import packages\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#import custom modules fpr testing\n",
    "import sys \n",
    "sys.path.append('.')\n",
    "import prep.prep_data as prep\n",
    "import model.fuzzy as fz\n",
    "\n",
    "#set globals for tests\n",
    "FILEPATH = '../data/example_data.csv'\n",
    "CLEAN_COLS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "\n",
    "DIGITS = str([str(x) for x in range(100 + 1)])\n",
    "PUNCT = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "SPACE = '     '\n",
    "\n",
    "STR_VARS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "NUM_VARS = [s + '_num' for s in STR_VARS]\n",
    "RANK_VARS = [s + '_rank' for s in STR_VARS]\n",
    "\n",
    "STR_GARBAGE = ['nan', 'other', 'not a dejure resident', 'not dejure resident']\n",
    "RANK_GARBAGE = ['4', '5', '6', '7', '8', '9', 'n']\n",
    "\n",
    "#read in example data using your function and then pass it through the cleaning pipeline\n",
    "df = prep.read_then_clean(FILEPATH, CLEAN_COLS)\n",
    "df_clean = prep.remove_garbage_codes(df, STR_VARS, STR_GARBAGE)\n",
    "df_clean = prep.extract_ranking(df_clean, NUM_VARS)\n",
    "df_clean = prep.remove_garbage_codes(df_clean, RANK_VARS, RANK_GARBAGE)\n",
    "\n",
    "#build corpus of known and unknown strings\n",
    "str_list, idk_strings = fz.build_corpus(df, base_var, rank_var, rank_values)\n",
    "\n",
    "def test_build_corpus():\n",
    "    \"\"\"This function tests a function that is used to build corpora of known and unknown words from a df \n",
    "    that contains columns with string value descriptions.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def test_fuzzy_scan():\n",
    "#find distribution of scores for each string\n",
    "distrib = fz.fuzzy_scan(idk_strings, str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FILEPATH = '../data/example_data.csv'\n",
    "CLEAN_COLS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "test_csv = pd.read_csv(FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "housing_roof nan\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "garbage values not present in the original data frame as expected",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-8a4075fbfc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#verify that it was originally there\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_csv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"garbage values not present in the original data frame as expected\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#assert that it is removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"garbage values not removed from clean dataframe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: garbage values not present in the original data frame as expected"
     ]
    }
   ],
   "source": [
    "for x in STR_VARS:\n",
    "    for y in STR_GARBAGE:\n",
    "        print(x, y)\n",
    "        #verify that it was originally there\n",
    "        assert (y in test_csv[x].unique()) == True, \"garbage values not present in the original data frame as expected\"\n",
    "        #assert that it is removed\n",
    "        assert (y in df_clean[x].unique()) == False, \"garbage values not removed from clean dataframe\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraps\n",
    "DIGITS = str([str(x) for x in range(100 + 1)])\n",
    "PUNCT = \"xx\"\n",
    "#PUNCT = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "SPACE = '     '\n",
    "\n",
    "(DIGITS)\n",
    "test_globals()\n",
    "#look at some of the clean values\n",
    "df_raw.housing_floor.unique().tolist()\n",
    "clean_text('32. vinyl_asphalt')\n",
    "print_vars = ['iso3', 'int_year', 'housing_roof', 'housing_roof_rank', 'housing_roof_rank_og', 'train']\n",
    "obj = train_list[1]\n",
    "obj[print_vars].sample(50)\n",
    "#command scraps\n",
    "#pd.crosstab(train_list[1]['housing_wall_rank'], columns='count')\n",
    "\n",
    "\n",
    "#fuzzy_scan scraps\n",
    "distrib = []\n",
    "match = []\n",
    "\n",
    "for x in range(len(idk_strings[1:50])): \n",
    "    unknown_string = idk_strings[x]\n",
    "    print('analyzing...', unknown_string)\n",
    "    \n",
    "    #set lists to store loop results\n",
    "    nat = []\n",
    "    rud = []\n",
    "    fin = []\n",
    "               \n",
    "    for y in range(len(nat_strings)):\n",
    "        nat.append(fuzz.WRatio(unknown_string, nat_strings[y]))\n",
    "        rud.append(fuzz.WRatio(unknown_string, rud_strings[y]))\n",
    "        fin.append(fuzz.WRatio(unknown_string, fin_strings[y]))\n",
    "    \n",
    "    #append distributions of scores\n",
    "    distrib.append(pd.DataFrame({'word': unknown_string, 'natural':nat, 'rudimentary':rud, 'finished':fin}))\n",
    "    \n",
    "    #pull best matches\n",
    "    #note that extractOne returns an array, first item is match/second the ratio\n",
    "    match.append(pd.DataFrame({'word': unknown_string, \n",
    "                               'natural':process.extractOne(unknown_string, nat_strings)[0], \n",
    "                               'rudimentary':process.extractOne(unknown_string, rud_strings)[0], \n",
    "                               'finished':process.extractOne(unknown_string, fin_strings)[0]}, \n",
    "                              index=[0])) \n",
    "\n",
    "distrib = pd.concat(distrib)\n",
    "distrib = fuzzy_transform(distrib, ['natural', 'rudimentary', 'finished'], 'word', 'sum', '_prob')\n",
    "match = pd.concat(match)\n",
    "\n",
    "str_list, idk_strings = fz.build_corpus(train_list[1], 'housing_roof', 'housing_roof_rank', ['1', '2', '3'])\n",
    "distrib = fz.fuzzy_scan(idk_strings, str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test globals\n",
    "DIGITS = str([str(x) for x in range(100 + 1)])\n",
    "PUNCT = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "SPACE = '     '\n",
    "\n",
    "# if you compile the regex string first, it's even faster\n",
    "re_dig = re.compile('\\d')\n",
    "re_punct = re.compile('\\W+')\n",
    "re_white = re.compile(' +')\n",
    "\n",
    "def test_globals():\n",
    "    \"\"\"This function tests that the test globals are properly defined.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(DIGITS) != None, \"Global doesn't contain digits!\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(PUNCT) != None, \"Global doesn't contain punctuation!\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(SPACE) != None, \"Global doesn't contain whitespace!\"\n",
    "    \n",
    "\n",
    "def test_clean_text():\n",
    "    \"\"\"This function tests that the clean text function is doing its job.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(prep.clean_text(DIGITS)) == None, \"clean_text did not remove the digits from test global.\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(prep.clean_text(PUNCT)) == None, \"clean_text did not remove the punctuation from test global.\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(prep.clean_text(SPACE)) == None, \"clean_text did not remove the whitespace from test global.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
