{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SANDBOX for code development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frostafarian/miniconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#import custom modules\n",
    "import prep.prep_data as prep\n",
    "import prep.prep_cv as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~begin reading\n",
      "data read!\n",
      "~begin cleaning\n",
      "data clean!\n",
      "~applying filter\n"
     ]
    }
   ],
   "source": [
    "df = prep.read_then_clean('../data/housing_data.csv',\n",
    "                          ['housing_roof', 'housing_wall', 'housing_floor'],\n",
    "                          ['MACRO_DHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nan': nan, 'other': nan, 'not a dejure resident': nan, 'not dejure resident': nan}\n",
      "removing garbage from  housing_roof\n",
      "removing garbage from  housing_wall\n",
      "removing garbage from  housing_floor\n",
      "defining ranking for  housing_roof_num\n",
      "defining ranking for  housing_wall_num\n",
      "defining ranking for  housing_floor_num\n",
      "{'4': nan, '5': nan, '6': nan, '7': nan, '8': nan, '9': nan, 'n': nan}\n",
      "removing garbage from  housing_roof_rank\n",
      "removing garbage from  housing_wall_rank\n",
      "removing garbage from  housing_floor_rank\n"
     ]
    }
   ],
   "source": [
    "df_clean = prep.remove_garbage_codes(df, \n",
    "                                     ['housing_roof', 'housing_wall', 'housing_floor'],\n",
    "                                     ['nan', 'other', 'not a dejure resident', 'not dejure resident'])\n",
    "df_clean = prep.extract_ranking(df_clean, ['housing_roof_num', 'housing_wall_num', 'housing_floor_num'])\n",
    "df_clean = prep.remove_garbage_codes(df_clean, \n",
    "                                     ['housing_roof_rank', 'housing_wall_rank', 'housing_floor_rank'],\n",
    "                                     ['4', '5', '6', '7', '8', '9', 'n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing_wall_rank</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>112616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>236884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0               count\n",
       "housing_wall_rank        \n",
       "1                   71026\n",
       "2                  112616\n",
       "3                  236884"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df_clean['housing_wall_rank'], columns='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to replace meaningless values with NaNs\n",
    "def extract_ranking(df, vars_to_clean):\n",
    "    \"\"\"This helper function is used to extract the ordinal rankings from numerical coding.\n",
    "\n",
    "    Args:\n",
    "    df (pandas df): This is a pandas df that has columns with garbage values to be removed.\n",
    "    vars_to_rank (list): This is a list of strings that indicate which columns you want to extract ranks from.\n",
    "\n",
    "    Returns:\n",
    "        df_out: This function returns a pandas df with new vars added with the ordinal rank cols defined.\n",
    "        \n",
    "    TODO: ?\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    \n",
    "    for var in vars_to_clean:\n",
    "        print(\"defining ranking for \", var)\n",
    "        newcol = re.sub(\"_num\", \"_rank\", var) \n",
    "        df_out[newcol] = df_out[var].astype(str).str[0]\n",
    "\n",
    "    #output a clean dataset\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cc3da37b1382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train'"
     ]
    }
   ],
   "source": [
    "pd.crosstab(train_list[1]['train'], columns='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling df, iteration # 0\n",
      "sampling df, iteration # 1\n",
      "sampling df, iteration # 2\n",
      "sampling df, iteration # 3\n",
      "sampling df, iteration # 4\n"
     ]
    }
   ],
   "source": [
    "train_list = cv.cv_censor_col(df_clean, 'housing_roof_rank', .2, 'N', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ihme_loc_id</th>\n",
       "      <th>nid</th>\n",
       "      <th>survey_series</th>\n",
       "      <th>hhweight</th>\n",
       "      <th>urban</th>\n",
       "      <th>hh_size</th>\n",
       "      <th>int_year</th>\n",
       "      <th>housing_roof</th>\n",
       "      <th>housing_wall</th>\n",
       "      <th>...</th>\n",
       "      <th>housing_wall_num</th>\n",
       "      <th>housing_floor_num</th>\n",
       "      <th>iso3</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>index</th>\n",
       "      <th>region_id</th>\n",
       "      <th>super_region_id</th>\n",
       "      <th>N</th>\n",
       "      <th>train</th>\n",
       "      <th>housing_roof_num_og</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56099.0</td>\n",
       "      <td>MACRO_DHS</td>\n",
       "      <td>695752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>cement bricks</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>AFG</td>\n",
       "      <td>28108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56099.0</td>\n",
       "      <td>MACRO_DHS</td>\n",
       "      <td>695752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>hay with mud</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>96</td>\n",
       "      <td>AFG</td>\n",
       "      <td>28108.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56099.0</td>\n",
       "      <td>MACRO_DHS</td>\n",
       "      <td>1075029.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>hay with mud</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>AFG</td>\n",
       "      <td>28108.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56099.0</td>\n",
       "      <td>MACRO_DHS</td>\n",
       "      <td>897294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>no walls</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>AFG</td>\n",
       "      <td>28108.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56099.0</td>\n",
       "      <td>MACRO_DHS</td>\n",
       "      <td>695752.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>nan</td>\n",
       "      <td>no walls</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>AFG</td>\n",
       "      <td>28108.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ihme_loc_id      nid survey_series   hhweight  urban  hh_size  \\\n",
       "0         1.0         AFG  56099.0     MACRO_DHS   695752.0    0.0      1.0   \n",
       "1         2.0         AFG  56099.0     MACRO_DHS   695752.0    0.0      1.0   \n",
       "2         3.0         AFG  56099.0     MACRO_DHS  1075029.0    0.0      1.0   \n",
       "3         4.0         AFG  56099.0     MACRO_DHS   897294.0    0.0      1.0   \n",
       "4         5.0         AFG  56099.0     MACRO_DHS   695752.0    0.0      1.0   \n",
       "\n",
       "   int_year housing_roof   housing_wall         ...          housing_wall_num  \\\n",
       "0    2010.0          nan  cement bricks         ...                        33   \n",
       "1    2010.0          nan   hay with mud         ...                        21   \n",
       "2    2010.0          nan   hay with mud         ...                        21   \n",
       "3    2010.0          nan       no walls         ...                        11   \n",
       "4    2010.0          nan       no walls         ...                        11   \n",
       "\n",
       "  housing_floor_num iso3 cluster_id index  region_id  super_region_id    N  \\\n",
       "0                41  AFG    28108.0   1.0      138.0            137.0  1.0   \n",
       "1                96  AFG    28108.0   2.0      138.0            137.0  1.0   \n",
       "2                13  AFG    28108.0   3.0      138.0            137.0  2.0   \n",
       "3                11  AFG    28108.0   4.0      138.0            137.0  1.0   \n",
       "4                32  AFG    28108.0   5.0      138.0            137.0  1.0   \n",
       "\n",
       "   train  housing_roof_num_og  \n",
       "0    0.0                   99  \n",
       "1    0.0                   96  \n",
       "2    0.0                   96  \n",
       "3    0.0                   96  \n",
       "4    0.0                   96  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[1].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at output datset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.housing_roof.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file ./prep/prep_cv.py\n",
    "\n",
    "#define necessary helper functions\n",
    "def cv_censor_col(df, colname, pct=.2, weight_var=None, reps=5):\n",
    "    \n",
    "    \"\"\"This function is used to create pandas dfs where a specified % of the values in a column have been censored\n",
    "    and replaced with NaN, so that they can be predicted in a cross-validation methodology. It returns a list of such\n",
    "    dfs that is the length of the reps argument.\n",
    "\n",
    "    Args:\n",
    "        df (pandas df): This is a pandas df that has columns with garbage values to be removed.\n",
    "        colname (str): This is a string indicating the name of a column that you want to censor and later predict.\n",
    "        pct (float): This is a value between 0-1 that indicates the fraction of values you want to censor. Default = 20%\n",
    "        weight_var (str): This is a string indicating the column name is used to weighted the sample. Default = No weight.\n",
    "        reps (int): This is an integer indicating the number of different training datasets to create. Default = 5x\n",
    "\n",
    "    Returns:\n",
    "        df_clean: This function returns a pandas df where the garbage codes have been replaced with NaN.\n",
    "        \n",
    "    TODO: ?\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #import packages\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for x in range(reps):\n",
    "            \n",
    "        print(\"sampling df, iteration #\", x)\n",
    "    \n",
    "        #first archive your old column in order to test later\n",
    "        new_df = df.copy()\n",
    "        new_df[colname + '_og'] = new_df[colname]\n",
    "        df['train'] = 1 #set column to specify whether training or test data\n",
    "\n",
    "        #draw a weighted sample if weight var is specified\n",
    "        if weight_var != None:\n",
    "            df_censor = new_df.sample(frac=pct, weights=weight_var)\n",
    "        else:\n",
    "            df_censor = new_df.sample(frac=pct)\n",
    "            \n",
    "        #now replace the sampled column with missing values in order to try and predict\n",
    "        #note that replacement is only done on the sampled indices\n",
    "        df['train'] = 0\n",
    "        df_censor[colname] = \"replace_me\"\n",
    "        new_df.update(df_censor, overwrite=True)\n",
    "        new_df[colname].replace(\"replace_me\", np.nan, inplace=True)\n",
    "        #TODO unsure if this is pythonic method but it seems like df.update won't replace values with NaN, \n",
    "        #as such, need to do this workaround\n",
    "        \n",
    "        #store the result (df with columns censored)\n",
    "        out.append(new_df)\n",
    "    \n",
    "    #return the list of sampled dfs\n",
    "    return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./prep/prep_data.py\n",
    "#define necessary helper functions\n",
    "def clean_text(text):\n",
    "    \"\"\"This function is used to clean a selection of text. \n",
    "    It uses several regular expressions and built in text commands in order to remove commonly seen \n",
    "    errors,\n",
    "    nonsense values, \n",
    "    punctuation, \n",
    "    digits, and \n",
    "    extra whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): This is a text value that needs to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        text: This function returns a cleaned version of the input text.\n",
    "        \n",
    "    TODO: Add functionality to impute a selected value for NaN or missing values?\n",
    "\n",
    "    \"\"\"\n",
    "    #import necessary modules\n",
    "    import re\n",
    "    \n",
    "    #force all vals in series to string\n",
    "    text = str(text)\n",
    "    \n",
    "    #first remove uppercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove common errors\n",
    "    text = re.sub(r\"\\[.]\", \"\", text) \n",
    "    text = re.sub(r\"\\<ff>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<fb>\", \"\", text)\n",
    "    text = re.sub(r\"\\<a\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<c\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<d\\d>\", \"\", text)\n",
    "    text = re.sub(r\"\\<e\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\<f\\d>\", \"\", text)   \n",
    "    text = re.sub(r\"\\d+\\.\", \"\", text)\n",
    "\n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)   \n",
    "\n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "    # remove any remaining digit codes\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # remove any leading/trailing/duplicate whitespace\n",
    "    text = re.sub(' +', ' ', text.strip())\n",
    "    \n",
    "    return text\n",
    "    \n",
    "#define master function\n",
    "def read_then_clean(file_path, vars_to_clean, filter_series=None):\n",
    "    \"\"\"This is the master function for this module. It uses the previously defined helper functions,\n",
    "    in order to output a clean dataset for user. It reads in a selected .csv file from a given filepath,\n",
    "    and applies the previously defined cleaning functions to a list of variables provided by user.\n",
    "    \n",
    "    It can also optionally filter the df based on the survey series or TODO language.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): This is a string indicating which file that you want to read in.\n",
    "        vars_to_clean (list): This is a list of strings that indicate which columns you want to clean.\n",
    "        filter_series (list): This is a list of strings that indicate which survey series to keep.\n",
    "\n",
    "    Returns:\n",
    "        df_clean: This is a pandas df that has columns of text values that have been cleaned using the helper function.\n",
    "        \n",
    "    TODO: Is it better to return an obj called df_clean to be more explicit to user?\n",
    "\n",
    "    \"\"\"\n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #read in your data\n",
    "    print(\"~begin reading\")\n",
    "    df_raw = pd.read_csv(file_path, low_memory=False)\n",
    "    min_nrow = len(df_raw) #save the row count to test after cleaning and verify that rows are not being dropped\n",
    "    print(\"data read!\")\n",
    "    \n",
    "    #cleanup\n",
    "    print(\"~begin cleaning\")\n",
    "    df_clean = df_raw.copy()\n",
    "    for var in vars_to_clean:\n",
    "        df_clean[var] = df_clean[var].apply(clean_text)\n",
    "    print(\"data clean!\")\n",
    "    \n",
    "    # Verify that the minimum rowcount continues to be met\n",
    "    if len(df_clean) < min_nrow:\n",
    "        class RowCountException(Exception):\n",
    "            \"\"\"Custom exception class.\n",
    "            \n",
    "            This exception is raised when the minimum row is unmet.\n",
    "\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        raise RowCountException(\"Minimum number of rows were not returned after cleaning. Data is being lost!\")\n",
    "        \n",
    "    # Filter data if filter arguments are provided by user\n",
    "    if filter_series != None:\n",
    "        print(\"~applying filter\")\n",
    "        df_clean = df_clean[df_clean['survey_series'].isin(filter_series)]\n",
    "        \n",
    "    #output a clean dataset\n",
    "    return df_clean\n",
    "\n",
    "#define function to replace meaningless values with NaNs\n",
    "def remove_garbage_codes(df, vars_to_clean, garbage_list):\n",
    "    \"\"\"This helper function is used to remove garbage values from a pandas df, replacing them with NaN.\n",
    "\n",
    "    Args:\n",
    "    df (pandas df): This is a pandas df that has columns with garbage values to be removed.\n",
    "    vars_to_clean (list): This is a list of strings that indicate which columns you want to clean.\n",
    "    garbage_list (list): This is a list of strings that indicate which garbage values to replace with NaN\n",
    "\n",
    "    Returns:\n",
    "        df_clean: This function returns a pandas df where the garbage codes have been replaced with NaN.\n",
    "        \n",
    "    TODO: ?\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # build dictionary to map all garbage values to NaN\n",
    "    garb_dict = {}\n",
    "    for string in garbage_list:\n",
    "        garb_dict[string] = np.nan\n",
    "    \n",
    "    print(garb_dict)\n",
    "    \n",
    "    for var in vars_to_clean:\n",
    "        print(\"removing garbage from \", var)\n",
    "        df_clean[var].replace(garb_dict, inplace=True)\n",
    "        \n",
    "    #output a clean dataset\n",
    "    return df_clean\n",
    "\n",
    "#define function to replace meaningless values with NaNs\n",
    "def extract_ranking(df, vars_to_clean):\n",
    "    \"\"\"This helper function is used to extract the ordinal rankings from numerical coding.\n",
    "\n",
    "    Args:\n",
    "    df (pandas df): This is a pandas df that has columns with garbage values to be removed.\n",
    "    vars_to_rank (list): This is a list of strings that indicate which columns you want to extract ranks from.\n",
    "\n",
    "    Returns:\n",
    "        df_out: This function returns a pandas df with new vars added with the ordinal rank cols defined.\n",
    "        \n",
    "    TODO: ?\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #import necessary modules\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    \n",
    "    df_out = df.copy()\n",
    "    \n",
    "    for var in vars_to_clean:\n",
    "        print(\"defining ranking for \", var)\n",
    "        newcol = re.sub(\"_num\", \"_rank\", var) \n",
    "        df_out[newcol] = df_out[var].astype(str).str[0]\n",
    "\n",
    "    #output a clean dataset\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ./tests/test_prep.py\n",
    "#write tests\n",
    "\"\"\"This is a module used to test a module: \"prep.py\" and its relevant functions read_then_clean and clean_text\n",
    "\n",
    "read_then_clean is a function that takes a csv with messy string values and \n",
    "creates then cleans a pandas df\n",
    "using clean_text\n",
    "\n",
    "This module tests that function by ensuring that it returns expected exceptions and\n",
    "does not contain unexpected values.\n",
    "\"\"\"\n",
    "# import packages\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#import custom modules fpr testing\n",
    "import prep.prep_data as prep\n",
    "\n",
    "#set globals for tests\n",
    "FILEPATH = '../data/housing_data.csv'\n",
    "CLEAN_COLS = ['housing_roof', 'housing_wall', 'housing_floor']\n",
    "DIGITS = str([str(x) for x in range(100 + 1)])\n",
    "PUNCT = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "SPACE = '     '\n",
    "\n",
    "# if you compile the regex string first, it's even faster\n",
    "re_dig = re.compile('\\d')\n",
    "re_punct = re.compile('\\W+')\n",
    "re_white = re.compile(' +')\n",
    "\n",
    "def test_globals():\n",
    "    \"\"\"This function tests that the test globals are properly defined.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(DIGITS) != None, \"global doesn't contain digits!\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(PUNCT) != None, \"global doesn't contain punctuation!\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(SPACE) != None, \"global doesn't contain whitespace!\"\n",
    "    \n",
    "\n",
    "def test_clean_text():\n",
    "    \"\"\"This function tests that the clean text function is doing its job.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(prep.clean_text(DIGITS)) == None, \"clean_text did not remove the digits from test global.\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(prep.clean_text(PUNCT)) == None, \"clean_text did not remove the punctuation from test global.\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(prep.clean_text(SPACE)) == None, \"clean_text did not remove the whitespace from test global.\"\n",
    "\n",
    "# This is our base dataset and it needs to be cleaned properly. The second argument specifies\n",
    "# the cols with string values that we want to be cleaned.\n",
    "\n",
    "\n",
    "#TODO, how to cause read_then_clean to raise the row count exception??\n",
    "def test_read_then_clean():\n",
    "    \"\"\"This function tests that a custom exception called RowCountException\n",
    "    will be returned when more than 1k rows are expected.\n",
    "    \"\"\"\n",
    "    with pytest.raises(Exception) as err:\n",
    "        test_df = prep.read_then_clean(FILEPATH,\n",
    "                                       CLEAN_COLS)\n",
    "    assert 'RowCountException' in str(err) #verify that your custom error is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraps\n",
    "DIGITS = str([str(x) for x in range(100 + 1)])\n",
    "PUNCT = \"xx\"\n",
    "#PUNCT = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "SPACE = '     '\n",
    "\n",
    "(DIGITS)\n",
    "test_globals()\n",
    "#look at some of the clean values\n",
    "df_raw.housing_floor.unique().tolist()\n",
    "clean_text('32. vinyl_asphalt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test globals\n",
    "DIGITS = str([str(x) for x in range(100 + 1)])\n",
    "PUNCT = '!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "SPACE = '     '\n",
    "\n",
    "# if you compile the regex string first, it's even faster\n",
    "re_dig = re.compile('\\d')\n",
    "re_punct = re.compile('\\W+')\n",
    "re_white = re.compile(' +')\n",
    "\n",
    "def test_globals():\n",
    "    \"\"\"This function tests that the test globals are properly defined.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(DIGITS) != None, \"Global doesn't contain digits!\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(PUNCT) != None, \"Global doesn't contain punctuation!\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(SPACE) != None, \"Global doesn't contain whitespace!\"\n",
    "    \n",
    "\n",
    "def test_clean_text():\n",
    "    \"\"\"This function tests that the clean text function is doing its job.\n",
    "    \"\"\"\n",
    "    #assert that digits are removed\n",
    "    assert re_dig.search(prep.clean_text(DIGITS)) == None, \"clean_text did not remove the digits from test global.\" \n",
    "    #assert that punctutation is removed\n",
    "    assert re_punct.search(prep.clean_text(PUNCT)) == None, \"clean_text did not remove the punctuation from test global.\"\n",
    "    #assert that excessive whitespace is removed\n",
    "    assert re_white.search(prep.clean_text(SPACE)) == None, \"clean_text did not remove the whitespace from test global.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
